{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from clpsych.store import Store\n",
    "from clpsych.helpers import load_tokens\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_liwc(path='./data/other_materials/liwc/LIWC2007.dic'):\n",
    "    reading_words = []\n",
    "    categories, words = {}, {}\n",
    "    with codecs.open(path, 'r') as fp:\n",
    "        # skip the very first '%'\n",
    "        fp.readline()\n",
    "        # read the data (first the categories and their indices, then the words)\n",
    "        for line in fp:\n",
    "            if line.strip() == '%':\n",
    "                reading_words = True\n",
    "            \n",
    "            if reading_words:\n",
    "                l = line.strip().split('\\t')\n",
    "                try:\n",
    "                    word, cats = l[0], [int(v) for v in l[1:]]\n",
    "                    # clear out the asterisk if there is one\n",
    "                    if word[-1] == '*':\n",
    "                        word = word[:-1]\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                # add it to the dict\n",
    "                words[word] = cats\n",
    "            else:\n",
    "                ix, cat = line.strip().split()\n",
    "                categories[int(ix)] = cat\n",
    "    return categories, words\n",
    "\n",
    "def process_text_with_liwc(text, words, subcats=[]):\n",
    "    topics = []\n",
    "    for token in text:\n",
    "        token_topics = words.get(token, [])\n",
    "        topics.extend(token_topics)\n",
    "    if len(subcats) > 0:\n",
    "        topics = [t for t in topics if t in subcats]\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories, words = read_liwc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_classes = pd.DataFrame.from_csv('data/classes/train_classes.txt')\n",
    "dev_classes = pd.DataFrame.from_csv('data/classes/dev_classes.txt')\n",
    "sample_classes = pd.DataFrame.from_csv('data/classes/sample_classes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = load_tokens('./data/tokens/lemmas.txt')\n",
    "df.replace(np.nan, '', regex=True)\n",
    "df['text_features'] = df[['title', 'doc']].astype(str).apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['topics'] = df.text_features.astype(str).str.split().apply(lambda x: process_text_with_liwc(x, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(lowercase=False, tokenizer=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vect.fit_transform(df['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized = pd.DataFrame(X.todense(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, vectorized], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in ['title', 'doc', 'topics', 'text_features']:\n",
    "    df = df.drop(col, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('data/features/liwc.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
