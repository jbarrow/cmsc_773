{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.collocations import *\n",
    "from clpsych.helpers import load_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(docs, stopwords):\n",
    "    \"\"\" Filter the stopwords from our dataset \"\"\"\n",
    "    filter_from = lambda d, s: [w for w in d if w not in s]\n",
    "    return [filter_from(doc, stopwords) for doc in docs]\n",
    "\n",
    "def load_stopwords(filename):\n",
    "    \"\"\" Load stopwords from a file and return the individual words as a set. \"\"\"\n",
    "    return set([s.strip() for s in open(filename).read().split('\\n')])\n",
    "\n",
    "def print_collocations(collocations):\n",
    "    \"\"\" Pretty-print the results. \"\"\"\n",
    "    for score in collocations:\n",
    "        print('{}\\t{}'.format(' '.join(score[0]), score[1]))\n",
    "\n",
    "def collocations(finder, fn, cutoff=None, max_count=None):\n",
    "    \"\"\" Get the top collocations using the NLTK finder \"\"\"\n",
    "    collocations = finder.score_ngrams(fn)\n",
    "    # return only those collocations above the defined cut-off\n",
    "    return [c for c in collocations[:max_count] if cutoff is None or c[1] >= cutoff]\n",
    "\n",
    "def load_docs():\n",
    "    \"\"\" Load the documents, and their desired labels. \"\"\"\n",
    "    sample_classes = pd.DataFrame.from_csv('data/classes/train_classes.txt')\n",
    "    # load all the tokens into a dataframe\n",
    "    df = load_tokens(mask='data/tokens/tokens?.txt')\n",
    "    # just get the documents we're looking at -- the samples in this case\n",
    "    sampled_docs = df.merge(sample_classes, on='post_id').replace(np.nan, '', regex=True)\n",
    "    # join the document and title values into one\n",
    "    sampled_docs['text_features'] = sampled_docs[['title', 'doc']].apply(lambda x: ' '.join(x), axis=1)\n",
    "    # return the relevant dataframe\n",
    "    return sampled_docs[['post_id', 'class', 'text_features']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = load_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_tokenized, all_tokenized = [], []\n",
    "for i, post_id, cls, text_features in docs.itertuples():\n",
    "    if cls:\n",
    "        positive_tokenized.append(text_features.strip().split())\n",
    "    all_tokenized.append(text_features.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = load_stopwords('data/stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_tokenized = filter_stopwords(positive_tokenized, stopwords)\n",
    "all_tokenized = filter_stopwords(all_tokenized, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_bgs = nltk.bigrams([t for doc in positive_tokenized for t in doc])\n",
    "cnt_bgs = nltk.bigrams([t for doc in all_tokenized for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "via ifttt](http://ift.tt/1gysbzm\t672030.753782\n",
      "trend score\t360626.184175\n",
      "% trend\t314893.953829\n",
      "subscriber today\t307070.27255\n",
      "feel like\t233904.507545\n",
      "mildly trending\t201009.119998\n",
      "right now\t153639.171609\n",
      "year old\t120942.480799\n",
      "anyone else\t118854.999615\n",
      "trend nsfw\t113755.062533\n",
      "oink oink\t95629.0204079\n",
      "year ago\t80161.1606728\n",
      "high school\t75492.4759511\n",
      "even though\t69280.0406925\n",
      "last night\t64875.1589085\n",
      "x post\t59417.5698779\n",
      "first time\t58770.9406145\n",
      "just want\t57200.927236\n",
      "pretty much\t52737.8900205\n",
      "hodor hodor\t52147.4206987\n",
      "month ago\t50853.0955312\n",
      "united states\t49798.6259698\n",
      "san diego\t49111.6157655\n",
      "look like\t47483.5965157\n",
      "need help\t45983.1391541\n",
      "seem like\t45331.9291669\n",
      "@ newegg\t43079.7074711\n",
      "new york\t42274.8145171\n",
      "every day\t39014.9136915\n",
      "come back\t37507.6360269\n",
      "every time\t36384.8964915\n",
      "100 %\t36227.3012295\n",
      "hey guy\t35903.8878248\n",
      "anyone know\t35752.5501148\n",
      "get rid\t35073.7745281\n",
      "los angeles\t34360.6776501\n",
      "greatly appreciate\t34313.1027664\n",
      "go back\t33431.1042318\n",
      "make sure\t33155.1451677\n",
      "long time\t32588.1539778\n",
      "week ago\t32238.3184747\n",
      "let know\t31966.1973354\n",
      "give away\t31705.8945254\n",
      "make sense\t31372.890669\n",
      "good friend\t31322.3283424\n",
      "show contact\t29893.531696\n",
      "video game\t29728.3306982\n",
      "thanks advance\t28756.0011076\n",
      "little bit\t28526.0760873\n",
      "power supply\t28126.5414803\n"
     ]
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = BigramCollocationFinder.from_documents(positive_tokenized)\n",
    "bigram_finder.apply_freq_filter(200)\n",
    "bigram_colls = collocations(\n",
    "    bigram_finder,\n",
    "    bigram_measures.,\n",
    "    max_count=50\n",
    ")\n",
    "print_collocations(bigram_colls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = BigramCollocationFinder.from_documents(all_tokenized)\n",
    "bigram_finder.apply_freq_filter(200)\n",
    "bigram_colls = collocations(\n",
    "    bigram_finder,\n",
    "    bigram_measures.likelihood_ratio,\n",
    "    max_count=50\n",
    ")\n",
    "print_collocations(bigram_colls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
