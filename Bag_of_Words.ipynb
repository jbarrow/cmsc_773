{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import roc_curve, accuracy_score, roc_auc_score\n",
    "from sklearn.decomposition import TruncatedSVD, IncrementalPCA\n",
    "from scipy.stats import chi2_contingency\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from clpsych.store import Store\n",
    "from clpsych.helpers import load_tokens\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import spacy\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_classes = pd.DataFrame.from_csv('data/classes/train_classes.txt')\n",
    "dev_classes = pd.DataFrame.from_csv('data/classes/dev_classes.txt')\n",
    "#sample_classes = pd.DataFrame.from_csv('data/classes/sample_classes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = load_tokens(mask='data/tokens/lemmas.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_docs = df.merge(train_classes, on='post_id').replace(np.nan, '', regex=True)\n",
    "test_docs = df.merge(dev_classes, on='post_id').replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_docs['text_features'] = labeled_docs[['title', 'doc']].apply(lambda x: ' '.join(x), axis=1)\n",
    "test_docs['text_features'] = test_docs[['title', 'doc']].apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  \n",
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "# clear out the big dataframe -- now we only need the subsets\n",
    "%reset_selective df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['text_features'] = df[['title', 'doc']].astype(str).apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=lambda x: x.split(),\n",
    "    stop_words='english', \n",
    "    decode_error='ignore', \n",
    "    min_df=40\n",
    ")\n",
    "\n",
    "transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer.fit(labeled_docs.text_features)\n",
    "vocab = vectorizer.transform(labeled_docs.text_features)\n",
    "transformer.fit(vocab)\n",
    "vocab = transformer.transform(vocab)\n",
    "test = transformer.transform(vectorizer.transform(test_docs.text_features))\n",
    "total = transformer.transform(vectorizer.transform(df.text_features))\n",
    "#test = vectorizer.transform(test_docs.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='arpack', n_components=100, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=100, algorithm='arpack')\n",
    "svd.fit(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = svd.transform(vocab)\n",
    "test = svd.transform(test)\n",
    "total = svd.transform(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n",
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset_selective vocab\n",
    "%reset_selective test\n",
    "\n",
    "pd.concat([df['post_id'], pd.DataFrame(total)], axis=1).to_csv('data/features/bow_100.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
