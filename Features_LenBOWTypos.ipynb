{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from clpsych import store, data\n",
    "import codecs\n",
    "import os\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data from parse files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#currently unused\n",
    "\n",
    "tokenized = {}\n",
    "i = 0\n",
    "\n",
    "base_i = i\n",
    "more = True\n",
    "while more:\n",
    "    if i % 10 == 0: print i\n",
    "    if i < base_i + 50 and os.path.isfile('parses/{}.parse'.format(i)):\n",
    "        docs, titles = read_parses('parses/{}.parse'.format(i))\n",
    "        keys = [key for key in docs.keys() if key]\n",
    "        for key in keys:\n",
    "            doc, title = [], []\n",
    "            for token in docs[key]:\n",
    "                doc.append(token)\n",
    "            for token in titles[key]:\n",
    "                title.append(token.lemma)\n",
    "            tokenized[key] = (title, doc)\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#including parse takes 6 hours and doesn't do anything\n",
    "config = {\n",
    "    'mask': './**/**/*.posts',\n",
    "    'train_mask': './**/**/TRAIN.txt',\n",
    "    'test_mask': './**/**/TEST.txt',\n",
    "    'dev_mask': './**/**/DEV.txt',\n",
    "    'sample_mask': './SAMPLE.txt',\n",
    "    #'parse':'./parses/*.parse',\n",
    "    #'parsed':True\n",
    "}\n",
    "\n",
    "load = store.Store('data.h5', config=config, overwrite = False)\n",
    "loaded_data = load.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this can be replaced with read_parses() if it saves \"tok.sents\" and \"tok.rank\"\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100138"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "indices = data.read_indices('SAMPLE.txt')\n",
    "load = store.Store('data.h5')\n",
    "loaded_data = load.select(indices)\n",
    "\n",
    "len(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed  0\n",
      "Completed  25000\n",
      "Completed  50000\n",
      "Completed  75000\n",
      "Completed  100000\n",
      "        user_id post_id  user_idnum_sent  avg_len  accuracy  freq_accuracy  \\\n",
      "0       -1014.0   wfp2i              3.0     12.0  0.973684       0.973684   \n",
      "1       -1014.0   wx3r4              4.0     21.0  1.000000       1.000000   \n",
      "2       -1014.0   x4hmq              0.0      0.0  1.000000       1.000000   \n",
      "3       -1014.0   xm156              0.0      0.0  1.000000       1.000000   \n",
      "4       -1014.0   xo2vm              0.0      0.0  1.000000       1.000000   \n",
      "5       -1014.0   yooo6              0.0      0.0  1.000000       1.000000   \n",
      "6       -1014.0   yy9w6              0.0      0.0  1.000000       1.000000   \n",
      "7       -1014.0   z4k8a              0.0      0.0  1.000000       1.000000   \n",
      "8       -1014.0   zg1e5              0.0      0.0  1.000000       1.000000   \n",
      "9       -1014.0  102okc              0.0      0.0  1.000000       1.000000   \n",
      "10      -1014.0  11xmvh              0.0      0.0  1.000000       1.000000   \n",
      "11      -1014.0  121z5t              0.0      0.0  1.000000       1.000000   \n",
      "12      -1014.0  12wnza              0.0      0.0  1.000000       1.000000   \n",
      "13      -1014.0  14983f              0.0      0.0  1.000000       1.000000   \n",
      "14      -1014.0  15gopu              0.0      0.0  1.000000       1.000000   \n",
      "15      -1014.0  1sdquo              3.0     46.0  0.956835       0.949640   \n",
      "16      -1245.0  2p56c5              7.0     28.0  0.989848       0.989848   \n",
      "17      -1245.0  2pjem6             14.0     10.0  0.979730       0.972973   \n",
      "18      -1245.0  2q6z3w              6.0     17.0  0.961905       0.952381   \n",
      "19      -1245.0  2r1zuy             13.0     14.0  0.984456       0.984456   \n",
      "20      -1245.0  2r51ad              5.0     18.0  0.967742       0.956989   \n",
      "21      -1245.0  2r5n4o              3.0     18.0  1.000000       1.000000   \n",
      "22      -1245.0  2rfkva             12.0     17.0  0.976636       0.967290   \n",
      "23      -1245.0  2s0u9s              9.0     15.0  0.992908       0.985816   \n",
      "24      -1245.0  2s6k85             23.0     13.0  0.993631       0.984076   \n",
      "25      -1245.0  2ujipr              9.0     21.0  0.974619       0.969543   \n",
      "26      -1245.0  2vt5af              0.0      0.0  1.000000       1.000000   \n",
      "27      -1245.0  2wtjqf              5.0      6.0  0.969697       0.969697   \n",
      "28      -1245.0  2x0e2m              3.0     20.0  0.966667       0.966667   \n",
      "29      -1245.0  2x26ba              8.0     10.0  0.976744       0.976744   \n",
      "...         ...     ...              ...      ...       ...            ...   \n",
      "100108  19870.0  2tr9ml             10.0     13.0  0.992366       0.984733   \n",
      "100109  19870.0  2unvsr              7.0     19.0  0.992593       0.962963   \n",
      "100110  19870.0  2z4qwk             10.0     14.0  1.000000       0.957447   \n",
      "100111  19870.0  33aj9i              5.0     15.0  0.986667       0.960000   \n",
      "100112  19870.0  3428qf              9.0     21.0  0.989637       0.968912   \n",
      "100113  19870.0  36662o              0.0      0.0  1.000000       1.000000   \n",
      "100114  19943.0   vvfrc              5.0     23.0  0.991597       0.983193   \n",
      "100115  19943.0   wl0ma              0.0      0.0  1.000000       1.000000   \n",
      "100116  19943.0   wqlbf             16.0     17.0  0.982206       0.978648   \n",
      "100117  19943.0   y0w7p              2.0     17.0  0.971429       0.971429   \n",
      "100118  19943.0   y1u19              1.0      5.0  0.600000       0.600000   \n",
      "100119  19943.0   z055w              0.0      0.0  1.000000       1.000000   \n",
      "100120  19943.0  13snhg              3.0     14.0  0.952381       0.952381   \n",
      "100121  19943.0  18og54              0.0      0.0  1.000000       1.000000   \n",
      "100122  19943.0  19eymx              0.0      0.0  1.000000       1.000000   \n",
      "100123  19943.0  1e02j2              0.0      0.0  1.000000       1.000000   \n",
      "100124  19943.0  1fftzg              0.0      0.0  1.000000       1.000000   \n",
      "100125  19943.0  1mjwbp              5.0     13.0  0.985507       0.956522   \n",
      "100126  19943.0  1mmnz1              2.0     20.0  0.975610       0.975610   \n",
      "100127  19943.0  1mzuzg              3.0     24.0  0.972222       0.958333   \n",
      "100128  19943.0  1nen0d              3.0     12.0  0.945946       0.945946   \n",
      "100129  19943.0  1nn18w              6.0     29.0  0.983240       0.983240   \n",
      "100130  19943.0  1oceml              6.0     33.0  0.980198       0.975248   \n",
      "100131  19943.0  1wcu7p              2.0     15.0  1.000000       0.933333   \n",
      "100132  19943.0  1wfd28              0.0      0.0  1.000000       1.000000   \n",
      "100133  19943.0  1wi8sp              0.0      0.0  1.000000       1.000000   \n",
      "100134  19943.0  1x5mj9             14.0     10.0  0.966216       0.966216   \n",
      "100135  19943.0  1y612s              0.0      0.0  1.000000       1.000000   \n",
      "100136  19943.0  1yi829              1.0      1.0  0.000000       0.000000   \n",
      "100137  19943.0  1yl390              0.0      0.0  1.000000       1.000000   \n",
      "\n",
      "        len_title  accuracy_title  freq_title  \n",
      "0             8.0        1.000000    1.000000  \n",
      "1            11.0        0.909091    0.818182  \n",
      "2             8.0        1.000000    0.750000  \n",
      "3            39.0        1.000000    1.000000  \n",
      "4             8.0        0.875000    0.750000  \n",
      "5             9.0        1.000000    1.000000  \n",
      "6            12.0        0.916667    0.833333  \n",
      "7             4.0        1.000000    1.000000  \n",
      "8             4.0        1.000000    1.000000  \n",
      "9            12.0        0.833333    0.833333  \n",
      "10           10.0        0.900000    0.900000  \n",
      "11            7.0        0.714286    0.714286  \n",
      "12            5.0        0.800000    0.800000  \n",
      "13           10.0        1.000000    1.000000  \n",
      "14            8.0        0.875000    0.750000  \n",
      "15           15.0        0.933333    0.933333  \n",
      "16            5.0        1.000000    1.000000  \n",
      "17            5.0        1.000000    1.000000  \n",
      "18            7.0        1.000000    1.000000  \n",
      "19            5.0        1.000000    1.000000  \n",
      "20            4.0        0.750000    0.750000  \n",
      "21            6.0        1.000000    1.000000  \n",
      "22            5.0        1.000000    1.000000  \n",
      "23            9.0        1.000000    1.000000  \n",
      "24            7.0        1.000000    1.000000  \n",
      "25            5.0        1.000000    1.000000  \n",
      "26            6.0        1.000000    1.000000  \n",
      "27            7.0        1.000000    1.000000  \n",
      "28            3.0        1.000000    1.000000  \n",
      "29            3.0        1.000000    1.000000  \n",
      "...           ...             ...         ...  \n",
      "100108        6.0        1.000000    1.000000  \n",
      "100109        7.0        1.000000    1.000000  \n",
      "100110        6.0        1.000000    1.000000  \n",
      "100111        4.0        1.000000    1.000000  \n",
      "100112        4.0        0.750000    0.750000  \n",
      "100113        3.0        1.000000    0.666667  \n",
      "100114       24.0        1.000000    1.000000  \n",
      "100115       14.0        1.000000    1.000000  \n",
      "100116       18.0        1.000000    1.000000  \n",
      "100117       10.0        1.000000    1.000000  \n",
      "100118       11.0        0.909091    0.909091  \n",
      "100119       16.0        1.000000    1.000000  \n",
      "100120       12.0        1.000000    1.000000  \n",
      "100121        7.0        1.000000    0.857143  \n",
      "100122        6.0        1.000000    1.000000  \n",
      "100123        8.0        1.000000    1.000000  \n",
      "100124        5.0        0.800000    0.800000  \n",
      "100125       10.0        1.000000    0.900000  \n",
      "100126       11.0        1.000000    0.909091  \n",
      "100127        9.0        1.000000    0.888889  \n",
      "100128        3.0        0.666667    0.666667  \n",
      "100129        9.0        0.888889    0.888889  \n",
      "100130       12.0        1.000000    1.000000  \n",
      "100131        9.0        1.000000    0.888889  \n",
      "100132        5.0        1.000000    0.800000  \n",
      "100133       16.0        1.000000    0.937500  \n",
      "100134        9.0        1.000000    1.000000  \n",
      "100135       11.0        1.000000    1.000000  \n",
      "100136        5.0        0.800000    0.800000  \n",
      "100137        8.0        1.000000    1.000000  \n",
      "\n",
      "[100138 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#this can be replaced with read_parses() if it saves \"tok.sents\"\n",
    "#currently takes ~3 hours to run\n",
    "\n",
    "\n",
    "df = pandas.DataFrame(columns=('user_id', 'post_id', 'num_sent', 'avg_len',  'accuracy', 'freq_accuracy', 'len_title', 'accuracy_title', 'freq_title'))\n",
    "#'spelling', 'freq', 'spelling_title', \n",
    "count = 0 \n",
    "for index, row in loaded_data.iterrows():\n",
    "    index = count #index, row in itterrows resets every 10k or so!\n",
    "    if (count%25000 == 0): print \"Completed \", count\n",
    "    #read with spacy, decode into unicode\n",
    "    spacy_doc = nlp(row['text'].decode(\"utf-8\"))\n",
    "    spacy_title = nlp(row['title'].decode(\"utf-8\"))\n",
    "    \n",
    "    #add tokens if their spacy rank isn't 0 (we could set a threshold here as well)\n",
    "    no_typos = [tok.text for tok in spacy_doc if tok.rank!=0]    \n",
    "    infreq = [tok.text for tok in spacy_doc if (tok.rank!=0 and tok.rank<100000)]    \n",
    "    \n",
    "    #simple counts (number of sentences, and avg lengh of sentence)\n",
    "    num_sent = len(list(spacy_doc.sents))\n",
    "    avg_len_sent =  sum([len(sent) for sent in list(spacy_doc.sents)])/len(list(spacy_doc.sents))  if len(list(spacy_doc.sents))> 0 else 0\n",
    "    \n",
    "    #most appear to have values over 90% (aka little typos).  Chose to set 0 length docs to 1.  \n",
    "    accuracy = float(len(no_typos))/len(spacy_doc) if len(spacy_doc) > 0 else 1\n",
    "    accuracy_infreq = float(len(infreq))/len(spacy_doc) if len(spacy_doc) > 0 else 1\n",
    "    \n",
    "    #do the same only for titles\n",
    "    no_typos_title = [tok.text for tok in spacy_title if tok.rank!=0]   \n",
    "    infreq_title = [tok.text for tok in spacy_title if (tok.rank!=0 and tok.rank<100000)]\n",
    "    \n",
    "    len_title =  len(spacy_title)\n",
    "    #most appear to have values over 90% (aka little typos).  Chose to set 0 length docs to 1.  \n",
    "    accuracy_title = float(len(no_typos_title))/len(spacy_title) if len(spacy_title) > 0 else 1\n",
    "    accuracy_infreq_title = float(len(infreq_title))/len(spacy_title) if len(spacy_title) > 0 else 1\n",
    "    \n",
    "    #append to new DF in the same row order\n",
    "    try:\n",
    "        df.loc[index] = row['user_id'], row['post_id'], num_sent, avg_len_sent, accuracy, accuracy_infreq, len_title, accuracy_title, accuracy_infreq_title\n",
    "        #no_typos, infreq,no_typos_title,\n",
    "    except:\n",
    "        print \"Issue with row \",  index\n",
    "        df.loc[index] = row['user_id'], row['post_id'], 0, 0, 0, 0, 0, 0\n",
    "    count+=1\n",
    "print df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['user_id','post_id', 'avg_len', 'accuracy', 'freq_accuracy','len_title','user_idnum_sent','freq_title','accuracy_title']].to_csv('sample_csv.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load = store.Store('data_Denis.h5', config=config, overwrite = False)\n",
    "data = load.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#doc\n",
    "load.create_document_feature(\"avg_len\", df[['post_id', 'avg_len']])\n",
    "load.create_document_feature(\"num_sent\", df[['post_id', 'num_sent']])\n",
    "load.create_document_feature(\"typo_acc\", df[['post_id', 'accuracy']])\n",
    "load.create_document_feature(\"freq_acc\", df[['post_id', 'freq_accuracy']])\n",
    "\n",
    "#title\n",
    "load.create_document_feature(\"len_title\", df[['post_id', 'len_title']])\n",
    "load.create_document_feature(\"typo_acc_title\", df[['post_id', 'accuracy_title']])\n",
    "load.create_document_feature(\"freq_acc_title\", df[['post_id', 'freq_title']])\n",
    "\n",
    "\n",
    "load.finalize\n",
    "load.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load.documents.to_csv(\"full_csv.csv\")\n",
    "\n",
    "indices = data.read_indices('SAMPLE.txt')\n",
    "load = store.Store('data.h5')\n",
    "df = load.select(indices, df=load.documents)\n",
    "df[['user_id','post_id','subreddit','avg_len', 'accuracy', 'freq_accuracy','len_title','num_sent','freq_title','accuracy_title']].to_csv('sample_csv.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#code for converting spelling into a long string.  This could be written into a DF if needed\n",
    "storage = []\n",
    "for row in df['spelling'][0:10]:\n",
    "    string = \"\"\n",
    "    for tok in row:\n",
    "        string = string+\" \" + tok.text\n",
    "    storage.append(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potpourri Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2594, 736, 683, 4, 8007, 405, 355, 49, 3075, 0, 264328, 80517, 436651, 0, 465]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#can we trust .rank?  I think yes.  High ranking words are pronouns, punctuation.  Low ranking words are proper nouns.  Misspellings are generally 0s (somethines high numbers)\n",
    "words = nlp(u'suicide death die I ugh hate myself me wtf asfab beelzebub shakespeare javert misppeling reddit')\n",
    "\n",
    "[tok.rank for tok in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
