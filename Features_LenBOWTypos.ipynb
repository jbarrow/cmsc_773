{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from clpsych import store, data\n",
    "import codecs\n",
    "import os\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data from parse files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "#currently unused\n",
    "\n",
    "tokenized = {}\n",
    "i = 0\n",
    "\n",
    "base_i = i\n",
    "more = True\n",
    "while more:\n",
    "    if i % 10 == 0: print i\n",
    "    if i < base_i + 50 and os.path.isfile('parses/{}.parse'.format(i)):\n",
    "        docs, titles = data.read_parses('parses/{}.parse'.format(i))\n",
    "    \n",
    "        keys = [key for key in docs.keys() if key]\n",
    "        for key in keys:\n",
    "            \n",
    "            doc, title = [], []\n",
    "            for token in docs[key]:\n",
    "                doc.append(token)\n",
    "            for token in titles[key]:\n",
    "                title.append(token)\n",
    "            tokenized[key] = (title, doc)\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#including parse takes 6 hours and doesn't do anything\n",
    "config = {\n",
    "    'mask': './**/**/*.posts',\n",
    "    'train_mask': './**/**/TRAIN.txt',\n",
    "    'test_mask': './**/**/TEST.txt',\n",
    "    'dev_mask': './**/**/DEV.txt',\n",
    "    'sample_mask': './SAMPLE.txt',\n",
    "    #'parse':'./parses/*.parse',\n",
    "    #'parsed':True\n",
    "}\n",
    "\n",
    "load = store.Store('data.h5', config=config, overwrite = False)\n",
    "loaded_data = load.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select subset of data\n",
    "#indices = data.read_indices('SAMPLE.txt')\n",
    "#load = store.Store('data.h5')\n",
    "#loaded_data = load.select(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "25000\n",
      "50000\n",
      "75000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pandas.DataFrame(columns=('post_id', 'num_sent', 'avg_len',  'accuracy', 'freq_accuracy', 'len_title', 'accuracy_title', 'freq_title'))\n",
    "#'spelling', 'freq', 'spelling_title', \n",
    "\n",
    "count = 0\n",
    "for key, val in tokenized.iteritems():\n",
    "    if count % 25000 == 0:\n",
    "        print count\n",
    "    \n",
    "    #read with spacy, decode into unicode\n",
    "    spacy_doc = val[1]\n",
    "    spacy_title = val[0]\n",
    "    \n",
    "    #add tokens if their spacy rank isn't 0 (we could set a threshold here as well)\n",
    "    no_typos = [tok[2] for tok in spacy_doc if tok[6]!=0]    \n",
    "    infreq = [tok[2] for tok in spacy_doc if (tok[6]!=0 and tok[6]<100000)]    \n",
    "    \n",
    "    \n",
    "    #most appear to have values over 90% (aka little typos).  Chose to set 0 length docs to 1.  \n",
    "    accuracy = float(len(no_typos))/len(spacy_doc) if len(spacy_doc) > 0 else 1\n",
    "    accuracy_infreq = float(len(infreq))/len(spacy_doc) if len(spacy_doc) > 0 else 1\n",
    "    \n",
    "    #do the same only for titles\n",
    "    no_typos_title = [tok[2] for tok in spacy_title if tok[6]!=0]   \n",
    "    infreq_title = [tok[2] for tok in spacy_title if (tok[6]!=0 and tok[6]<100000)]\n",
    "    \n",
    "    len_title =  len(spacy_title)\n",
    "    #most appear to have values over 90% (aka little typos).  Chose to set 0 length docs to 1.  \n",
    "    accuracy_title = float(len(no_typos_title))/len(spacy_title) if len(spacy_title) > 0 else 1\n",
    "    accuracy_infreq_title = float(len(infreq_title))/len(spacy_title) if len(spacy_title) > 0 else 1\n",
    "    \n",
    "    #simple counts (number of sentences, and avg lengh of sentence)\n",
    "    num_sent = [col[5] for col in spacy_doc].count(0)\n",
    "    avg_len_sent =  len(spacy_doc)/num_sent if num_sent>0 else 0\n",
    "    \n",
    "    #append to new DF in the same row order\n",
    "    try:\n",
    "        df.loc[count] = key, num_sent, avg_len_sent, accuracy, accuracy_infreq, len_title, accuracy_title, accuracy_infreq_title\n",
    "        #no_typos, infreq,no_typos_title,\n",
    "    except:\n",
    "        print \"Issue with row \",  key\n",
    "        df.loc[count] = key, 0, 0, 0, 0, 0, 0, 0\n",
    "    count+=1 \n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['user_id','post_id', 'avg_len', 'accuracy', 'freq_accuracy','len_title','num_sent','freq_title','accuracy_title']].to_csv('full_csv.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Save into h5 - Update this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load = store.Store('data_features.h5', config=config, overwrite = False)\n",
    "data = load.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#doc\n",
    "load.create_document_feature(\"avg_len\", df[['post_id', 'avg_len']])\n",
    "load.create_document_feature(\"num_sent\", df[['post_id', 'num_sent']])\n",
    "load.create_document_feature(\"typo_acc\", df[['post_id', 'accuracy']])\n",
    "load.create_document_feature(\"freq_acc\", df[['post_id', 'freq_accuracy']])\n",
    "\n",
    "#title\n",
    "load.create_document_feature(\"len_title\", df[['post_id', 'len_title']])\n",
    "load.create_document_feature(\"typo_acc_title\", df[['post_id', 'accuracy_title']])\n",
    "load.create_document_feature(\"freq_acc_title\", df[['post_id', 'freq_title']])\n",
    "\n",
    "\n",
    "load.finalize\n",
    "load.documents\n",
    "\n",
    "#load.documents.to_csv(\"full_csv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potpourri Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#can we trust .rank?  I think yes.  High ranking words are pronouns, punctuation.  Low ranking words are proper nouns.  Misspellings are generally 0s (somethines high numbers)\n",
    "words = nlp(u'suicide death die I ugh hate myself me wtf asfab beelzebub shakespeare javert misppeling reddit')\n",
    "\n",
    "[tok.rank for tok in words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legacy code to run without parses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#this can be replaced with read_parses() if it saves \"tok.sents\"\n",
    "#currently takes ~3 hours to run\n",
    "\n",
    "\n",
    "df = pandas.DataFrame(columns=('user_id', 'post_id', 'num_sent', 'avg_len',  'accuracy', 'freq_accuracy', 'len_title', 'accuracy_title', 'freq_title'))\n",
    "#'spelling', 'freq', 'spelling_title', \n",
    "count = 0 \n",
    "for index, row in loaded_data.iterrows():\n",
    "    index = count #index, row in itterrows resets every 10k or so!\n",
    "    if (count%25000 == 0): print \"Completed \", count\n",
    "    #read with spacy, decode into unicode\n",
    "    spacy_doc = nlp(row['text'].decode(\"utf-8\"))\n",
    "    spacy_title = nlp(row['title'].decode(\"utf-8\"))\n",
    "    \n",
    "    #add tokens if their spacy rank isn't 0 (we could set a threshold here as well)\n",
    "    no_typos = [tok.text for tok in spacy_doc if tok.rank!=0]    \n",
    "    infreq = [tok.text for tok in spacy_doc if (tok.rank!=0 and tok.rank<100000)]    \n",
    "    \n",
    "    #simple counts (number of sentences, and avg lengh of sentence)\n",
    "    num_sent = len(list(spacy_doc.sents))\n",
    "    avg_len_sent =  sum([len(sent) for sent in list(spacy_doc.sents)])/len(list(spacy_doc.sents))  if len(list(spacy_doc.sents))> 0 else 0\n",
    "    \n",
    "    #most appear to have values over 90% (aka little typos).  Chose to set 0 length docs to 1.  \n",
    "    accuracy = float(len(no_typos))/len(spacy_doc) if len(spacy_doc) > 0 else 1\n",
    "    accuracy_infreq = float(len(infreq))/len(spacy_doc) if len(spacy_doc) > 0 else 1\n",
    "    \n",
    "    #do the same only for titles\n",
    "    no_typos_title = [tok.text for tok in spacy_title if tok.rank!=0]   \n",
    "    infreq_title = [tok.text for tok in spacy_title if (tok.rank!=0 and tok.rank<100000)]\n",
    "    \n",
    "    len_title =  len(spacy_title)\n",
    "    #most appear to have values over 90% (aka little typos).  Chose to set 0 length docs to 1.  \n",
    "    accuracy_title = float(len(no_typos_title))/len(spacy_title) if len(spacy_title) > 0 else 1\n",
    "    accuracy_infreq_title = float(len(infreq_title))/len(spacy_title) if len(spacy_title) > 0 else 1\n",
    "    \n",
    "    #append to new DF in the same row order\n",
    "    try:\n",
    "        df.loc[index] = row['user_id'], row['post_id'], num_sent, avg_len_sent, accuracy, accuracy_infreq, len_title, accuracy_title, accuracy_infreq_title\n",
    "        #no_typos, infreq,no_typos_title,\n",
    "    except:\n",
    "        print \"Issue with row \",  index\n",
    "        df.loc[index] = row['user_id'], row['post_id'], 0, 0, 0, 0, 0, 0\n",
    "    count+=1\n",
    "print df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#code for converting spelling into a long string.  This could be written into a DF if needed\n",
    "storage = []\n",
    "for row in df['spelling'][0:10]:\n",
    "    string = \"\"\n",
    "    for tok in row:\n",
    "        string = string+\" \" + tok.text\n",
    "    storage.append(string)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
