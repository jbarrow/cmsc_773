{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": 76,
>>>>>>> b292da52f228bb12f790a128611e9039bacc8634
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from clpsych import store, data\n",
<<<<<<< HEAD
    "import codecs\n",
    "import os\n",
=======
    "\n",
    "import codecs\n",
    "import os\n",
    "\n",
>>>>>>> b292da52f228bb12f790a128611e9039bacc8634
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data from parse files (not Spacy format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "#currently unused\n",
    "\n",
=======
>>>>>>> b292da52f228bb12f790a128611e9039bacc8634
    "tokenized = {}\n",
    "i = 0\n",
    "\n",
    "base_i = i\n",
    "more = True\n",
    "while more:\n",
    "    if i % 10 == 0: print i\n",
    "    if i < base_i + 50 and os.path.isfile('parses/{}.parse'.format(i)):\n",
    "        docs, titles = read_parses('parses/{}.parse'.format(i))\n",
    "        keys = [key for key in docs.keys() if key]\n",
    "        for key in keys:\n",
    "            doc, title = [], []\n",
    "            for token in docs[key]:\n",
    "                doc.append(token)\n",
    "            for token in titles[key]:\n",
    "                title.append(token.lemma)\n",
    "            tokenized[key] = (title, doc)\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data.h5"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
=======
   "execution_count": 90,
>>>>>>> b292da52f228bb12f790a128611e9039bacc8634
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#including parse takes 6 hours and doesn't do anything\n",
    "config = {\n",
    "    'mask': './**/**/*.posts',\n",
    "    'train_mask': './**/**/TRAIN.txt',\n",
    "    'test_mask': './**/**/TEST.txt',\n",
    "    'dev_mask': './**/**/DEV.txt',\n",
    "    'sample_mask': './SAMPLE.txt',\n",
    "    #'parse':'./parses/*.parse',\n",
    "    #'parsed':True\n",
    "}\n",
    "\n",
    "load = store.Store('data.h5', config=config, overwrite = False)\n",
<<<<<<< HEAD
    "loaded_data = load.data"
=======
    "data = load.data"
>>>>>>> b292da52f228bb12f790a128611e9039bacc8634
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Lengths"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> b292da52f228bb12f790a128611e9039bacc8634
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this can be replaced with read_parses() if it saves \"tok.sents\" and \"tok.rank\"\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this can be replaced with read_parses() if it saves \"tok.sents\"\n",
    "#currently takes ~3 hours to run\n",
    "\n",
<<<<<<< HEAD
    "df = pandas.DataFrame(columns=('post_id', 'num_sent', 'avg_len',  'accuracy', 'freq_accuracy', 'len_title', 'accuracy_title', 'freq_title'))\n",
    "#'spelling', 'freq', 'spelling_title', \n",
    "count = 0 \n",
    "for index, row in loaded_data.iterrows():\n",
    "    index = count #index, row in itterrows resets every 10k or so!\n",
    "    #read with spacy, decode into unicode\n",
    "    spacy_doc = nlp(row['text'].decode(\"utf-8\"))\n",
    "    spacy_title = nlp(row['title'].decode(\"utf-8\"))\n",
    "    \n",
    "    #add tokens if their spacy rank isn't 0 (we could set a threshold here as well)\n",
    "    no_typos = [tok.text for tok in spacy_doc if tok.rank!=0]    \n",
    "    infreq = [tok.text for tok in spacy_doc if (tok.rank!=0 and tok.rank<100000)]    \n",
=======
    "df = pandas.DataFrame(columns=('post_id', 'num_sent', 'avg_len', 'spelling', 'accuracy'))\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    #read with spacy, decode into unicode\n",
    "    spacy_doc = nlp(row['text'].decode(\"utf-8\"))\n",
    "    \n",
    "    #add tokens if their spacy rank isn't 0 (we could set a threshold here as well)\n",
    "    no_typos = [tok for tok in spacy_doc if tok.rank!=0]    \n",
>>>>>>> b292da52f228bb12f790a128611e9039bacc8634
    "    \n",
    "    #simple counts (number of sentences, and avg lengh of sentence)\n",
    "    num_sent = len(list(spacy_doc.sents))\n",
    "    avg_len_sent =  sum([len(sent) for sent in list(spacy_doc.sents)])/len(list(spacy_doc.sents))  if len(list(spacy_doc.sents))> 0 else 0\n",
    "    \n",
    "    #most appear to have values over 90% (aka little typos).  Chose to set 0 length docs to 1.  \n",
    "    accuracy = float(len(no_typos))/len(spacy_doc) if len(spacy_doc) > 0 else 1\n",
<<<<<<< HEAD
    "    accuracy_infreq = float(len(infreq))/len(spacy_doc) if len(spacy_doc) > 0 else 1\n",
    "    \n",
    "    #do the same only for titles\n",
    "    no_typos_title = [tok.text for tok in spacy_title if tok.rank!=0]   \n",
    "    infreq_title = [tok.text for tok in spacy_title if (tok.rank!=0 and tok.rank<100000)]\n",
    "    \n",
    "    len_title =  len(spacy_title)\n",
    "    #most appear to have values over 90% (aka little typos).  Chose to set 0 length docs to 1.  \n",
    "    accuracy_title = float(len(no_typos_title))/len(spacy_title) if len(spacy_title) > 0 else 1\n",
    "    accuracy_infreq_title = float(len(infreq_title))/len(spacy_title) if len(spacy_title) > 0 else 1\n",
    "    \n",
    "    #append to new DF in the same row order\n",
    "    try:\n",
    "        df.loc[index] = row['post_id'], num_sent, avg_len_sent, accuracy, accuracy_infreq, len_title, accuracy_title, accuracy_infreq_title\n",
    "        #no_typos, infreq,no_typos_title,\n",
    "    except:\n",
    "        print \"Issue with row \",  index\n",
    "        df.loc[index] = row['post_id'], 0, 0, 0, 0, 0, 0\n",
    "    count+=1\n",
=======
    "    \n",
    "    #append to new DF in the same row order\n",
    "    df.loc[index] = row['post_id'], num_sent, avg_len_sent, no_typos, accuracy\n",
    "\n",
>>>>>>> b292da52f228bb12f790a128611e9039bacc8634
    "print df\n",
    "    "
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load = store.Store('data_Denis.h5', config=config, overwrite = False)\n",
    "data = load.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#doc\n",
    "load.create_document_feature(\"avg_len\", df[['post_id', 'avg_len']])\n",
    "load.create_document_feature(\"num_sent\", df[['post_id', 'num_sent']])\n",
    "load.create_document_feature(\"typo_acc\", df[['post_id', 'accuracy']])\n",
    "load.create_document_feature(\"freq_acc\", df[['post_id', 'freq_accuracy']])\n",
    "\n",
    "#title\n",
    "load.create_document_feature(\"len_title\", df[['post_id', 'len_title']])\n",
    "load.create_document_feature(\"typo_acc_title\", df[['post_id', 'accuracy_title']])\n",
    "load.create_document_feature(\"freq_acc_title\", df[['post_id', 'freq_title']])\n",
    "\n",
    "\n",
    "#adding the spacy tokens in \"spelling\" causes load.documents to break, so left it out\n",
    "\n",
    "load.finalize\n",
    "load.documents"
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO BoW BoDependencies, Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save "
>>>>>>> b292da52f228bb12f790a128611e9039bacc8634
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "load.documents.to_csv(\"full_csv.csv\")\n",
    "\n",
    "indices = data.read_indices('SAMPLE.txt')\n",
    "load = store.Store('data.h5')\n",
    "df = load.select(indices, df=load.documents)\n",
    "df[['user_id','post_id','subreddit','avg_len', 'accuracy', 'freq_accuracy','len_title','num_sent','freq_title','accuracy_title']].to_csv('sample_csv.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#code for converting spelling into a long string.  This could be written into a DF if needed\n",
    "storage = []\n",
    "for row in df['spelling'][0:10]:\n",
    "    string = \"\"\n",
    "    for tok in row:\n",
    "        string = string+\" \" + tok.text\n",
    "    storage.append(string)"
=======
    "store.create_doc_feature(store.data ,df['post_id', 'num_sent', 'avg_len', 'spelling', 'accuracy'])"
>>>>>>> b292da52f228bb12f790a128611e9039bacc8634
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potpourri Analysis"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
=======
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2594, 736, 683, 4, 8007, 405, 355, 49, 3075, 0, 264328, 80517, 436651, 0, 465]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> b292da52f228bb12f790a128611e9039bacc8634
   "source": [
    "#can we trust .rank?  I think yes.  High ranking words are pronouns, punctuation.  Low ranking words are proper nouns.  Misspellings are generally 0s (somethines high numbers)\n",
    "words = nlp(u'suicide death die I ugh hate myself me wtf asfab beelzebub shakespeare javert misppeling reddit')\n",
    "\n",
    "[tok.rank for tok in words]\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
